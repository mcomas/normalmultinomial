---
title: "Likelihood estimation"
author: "Marc Comas-CufÃ­"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Likelihood estimation}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r}
library(normalmultinomial)
```

The normal-multinomial distribution is the distribution resulting after compounding the normal distribution defined on the Simplex and the well-known multinomial distribution. The parameters of a normal-multinomial distribution are the mean and covariance, $\mu$ and $\Sigma$, which define the parameters of the underlying normal distribution.

For example, if we consider the parameters:

```{r}
mu = c(0, 0)
Sigma = matrix(c(1, 0,
                 0, 1), nrow=2)
```

```{r}
rnormalmultinomial(mu = mu, sigma = Sigma, size = c(10, 100))
```







The probability density function of a Normal-multinomial distribution is given by

\[
P(X = (x_1, \dots, x_K)) = \int_{a \in R^K} \frac{(\sum x_i)!}{\prod x_i!} \prod_{1} p_i^{x_i} dALN(p; \mu, \Sigma).
\]

Fitting the parameters $(\mu, \Sigma)$ using maximum likelihood it is not straightforward difficult because it is required to optimize $P(X = (x_1, \dots, x_K))$ with respect $\mu$ and $\Sigma$.

For an observation $X = (x_1, \dots, x_K)$, assuming we know the value of probability $Z = (p_1, \dots, p_K) \sim ALN(\mu, \Sigma)$. The likelihoof of the join distribution of $X$ and $Z$ can be written as

\[
P(X , Z ) = P(X  | Z) P(Z).
\]

\[
(2 \pi)^{-k/2}|\Sigma|^{-\frac{1}{2}} exp(-\frac{1}{2} (alr(p)-\mu)^{T} \Sigma^{-1} (alr(p)-\mu)) \frac{(x_1+\dots+x_K)!}{x_1! \dots x_K!}\prod_{\ell=1}^K p_\ell^{x_\ell}
\]

## Likelihood estimation 

The reduced likelihood function condition to unobserved $a = [a_1, ..., a_k] := alr([c_1, ..., c_K]) = [log(c_1/c_K), ..., log(c_k/c_K)]$ where $K = k +1$ is


\[
l(a) = -\frac{1}{2} (a-\mu)^{T} \Sigma^{-1} (a-\mu) + X_K log(\frac{1}{\kappa}) + \sum_{\ell = 1}^k X_\ell log(\frac{e^{a_\ell}}{\kappa}) 
\]

where $\kappa = 1 + e^{a_1} + \dots + e^{a_k}$.

#### First and second derivatives

\[
\frac{\partial l(a)}{\partial a_i} = - \sum_{\ell=1}^k (a_i-\mu_i) \Sigma^{-1}_{(i,\ell)} 
+ X_i \; \frac{\kappa-e^{a_i}}{\kappa} 
+ \sum_{\ell=1, \ell \neq i}^K X_{\ell}\; \frac{-e^{a_i}}{\kappa} 
\]

\[
\frac{\partial^2 l(a)}{\partial a_i^2} = -\Sigma^{-1}_{(i,i)} - \sum_{\ell=1}^K x_\ell \frac{e^{a_i} (\kappa - e^{a_i})}{\kappa^2}
\]

\[
\frac{\partial^2 l(a)}{\partial a_i \partial a_j} = -\Sigma^{-1}_{(i,j)} + \sum_{\ell=1}^K x_\ell \frac{e^{a_i+a_j}}{\kappa^2}
\]